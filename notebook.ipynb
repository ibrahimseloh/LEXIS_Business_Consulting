{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "047df765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fofana-ibrahim-seloh/NEW/chat-with-your-data-geminy/chat_data/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured_pytesseract.pytesseract import TesseractError\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import List, Dict\n",
    "import tempfile\n",
    "import os\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured_pytesseract.pytesseract import TesseractError\n",
    "\n",
    "def extract_filename(filepath: str) -> str:\n",
    "    \"\"\"Extrait le nom du fichier sans extension depuis le chemin complet\"\"\"\n",
    "    return os.path.splitext(os.path.basename(filepath))[0]\n",
    "\n",
    "def ocr_pipeline(pdf_path: str) -> List[Dict[str, str]]:\n",
    "    doc_name = extract_filename(pdf_path)\n",
    "    elements = []\n",
    "    \n",
    "    # Lire le PDF original et séparer les pages\n",
    "    reader = PdfReader(pdf_path)\n",
    "    total_pages = len(reader.pages)\n",
    "\n",
    "    for page_num in range(total_pages):\n",
    "        # Créer un PDF temporaire pour chaque page\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_file:\n",
    "            writer = PdfWriter()\n",
    "            writer.add_page(reader.pages[page_num])\n",
    "            writer.write(temp_file)\n",
    "            temp_file_path = temp_file.name\n",
    "\n",
    "        try:\n",
    "            # Traiter la page individuelle avec OCR\n",
    "            with open(temp_file_path, \"rb\") as f:\n",
    "                raw = partition_pdf(\n",
    "                    file=f,\n",
    "                    ocr_languages=\"fra+eng\",\n",
    "                    ocr_strategy=\"auto\",\n",
    "                    infer_table_structure=True,\n",
    "                    extract_images_in_pdf=True,\n",
    "                    pdf_image_dpi=300,\n",
    "                    max_characters=4000,\n",
    "                    new_after_n_chars=3800,\n",
    "                    combine_text_under_n_chars=2000,\n",
    "                )\n",
    "        except TesseractError:\n",
    "            # Fallback en cas d'erreur OCR\n",
    "            with open(temp_file_path, \"rb\") as f:\n",
    "                raw = partition_pdf(\n",
    "                    file=f,\n",
    "                    strategy=\"fast\",\n",
    "                    infer_table_structure=True,\n",
    "                )\n",
    "        finally:\n",
    "            os.unlink(temp_file_path)  # Nettoyer le fichier temporaire\n",
    "\n",
    "        # Extraire le contenu de la page\n",
    "        content = \"\\n\".join(\n",
    "            [elem.get_text() if hasattr(elem, \"get_text\") else getattr(elem, \"text\", \"\") \n",
    "            for elem in raw]\n",
    "        )\n",
    "        \n",
    "        elements.append({\n",
    "            \"id\": f\"chunk_{page_num + 1}_{doc_name}\",\n",
    "            \"page\": page_num + 1,\n",
    "            \"text\": content,\n",
    "            \"doc\": doc_name\n",
    "        })\n",
    "    \n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2233d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import google.generativeai as genai\n",
    "\n",
    "def index_pdf_elements(elements, api_key: str, collection_name: str) -> Chroma:\n",
    "    genai.configure(api_key=api_key)\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)\n",
    "    \n",
    "    # Préparer les documents pour l'indexation\n",
    "    docs = [\n",
    "        Document(\n",
    "            page_content=el[\"text\"], \n",
    "            metadata={\n",
    "                \"id\": el[\"id\"],\n",
    "                \"page\": el[\"page\"],\n",
    "                \"doc\": el[\"doc\"],\n",
    "            }\n",
    "        ) for el in elements\n",
    "    ]\n",
    "    \n",
    "    # Créer ou mettre à jour le vectorstore\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=\".chroma_db\"\n",
    "    )\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5e1970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in ./chat_data/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./chat_data/lib/python3.10/site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./chat_data/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./chat_data/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./chat_data/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./chat_data/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./chat_data/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/fofana-ibrahim-seloh/NEW/chat-with-your-data-geminy/chat_data/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41cdbaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def get_final_sources_used(response, docs):\n",
    "    response = response.replace(\",\", \"\")\n",
    "    lines = response.splitlines()\n",
    "    source_indices = set()\n",
    "    final_sources = []\n",
    "\n",
    "    for line in lines:\n",
    "        numbers = re.findall(r'\\d+', line)\n",
    "        for n in numbers:\n",
    "            source_indices.add(int(n))\n",
    "\n",
    "    for i in sorted(source_indices):\n",
    "        if 0 < i <= len(docs): \n",
    "            doc = docs[i - 1]\n",
    "            doc['id'] = str(i)\n",
    "            final_sources.append(doc)\n",
    "\n",
    "    return final_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce576e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "MAIN_TEMPLATE = \"\"\"\n",
    "You are *Lexis*, a strategic consulting expert focused on providing high-level advisory services for business dossiers used in tenders and public procurement. Your role is to assist clients by analyzing critical documents such as technical specifications, pricing models, and administrative clauses, offering insights that enhance tender strategies and decision-making.\n",
    "\n",
    "### Key Points of Your Mission:\n",
    "- **Strategic Analysis and Insight**:  \n",
    "    - Your primary task is to analyze business dossiers, extracting key information from technical, pricing, and administrative documents to provide actionable, strategic recommendations.\n",
    "    - Focus on identifying opportunities for competitive advantage while ensuring compliance with procurement guidelines.\n",
    "    - Highlight any outdated or irrelevant documents without unnecessary explanation, ensuring your advice is based on current, applicable standards.\n",
    "\n",
    "- **Clarity and Professionalism in Communication**:  \n",
    "    - Deliver responses that are clear, structured, and tailored to the needs of business leaders and legal professionals involved in the tender process.\n",
    "    - Use a logical flow with headings, subheadings, and bullet points, presenting complex information in an easily digestible format.\n",
    "\n",
    "- **Thorough, Practical, and Actionable Guidance**:  \n",
    "    - Provide in-depth, yet practical, advice that is directly applicable to the client’s business strategy and tender process.\n",
    "    - Simplify intricate concepts when necessary, without compromising on accuracy or strategic value, ensuring your insights are easily actionable.\n",
    "\n",
    "- **Alignment with Business Objectives**:  \n",
    "    - Always prioritize the most relevant documents and guidelines that align with the client’s strategic goals in the tendering process.\n",
    "    - Offer recommendations that focus on improving the client’s competitiveness and ensuring alignment with procurement regulations, driving overall success in the bidding process.\n",
    "\n",
    "## Formatting Instructions\n",
    "- **Structure**: Organize your response logically with clear, descriptive headings (e.g., \"## Example Heading 1\" or \"## Example Heading 2\"). Present key points using concise paragraphs or bullet points for better readability and impact.\n",
    "- **Markdown Usage**: Use Markdown effectively to enhance clarity. Employ **bold** to emphasize critical terms, *italics* for supplementary explanations or clarifications, and headers to structure the content clearly and logically.\n",
    "- **No Main Title**: Start directly with the body of the response, unless a specific title is requested. Keep the flow natural and direct.\n",
    "- **Conclusion or Summary**: Wrap up with a concise conclusion or actionable next steps, guiding the client on how to refine their strategy or secure additional information for the tender process.\n",
    "\n",
    "- **Markdown**:  \n",
    "    - Use **bold** for essential terms or concepts, *italics* for clarifications, and headers to divide the content for easy reference and navigation.\n",
    "\n",
    "- **Conclusion**:  \n",
    "    - Conclude with a focused summary, restating key insights, or recommend immediate actions the client should take, such as refining their submission or acquiring the necessary documentation for the next steps in the process.\n",
    "\n",
    "## Citation Requirements\n",
    "- Cite every fact, statement, or phrase using the notation [number] corresponding to the source provided in the sources.\n",
    "- Integrate citations naturally at the end of sentences or clauses, as appropriate. For example: \"The Eiffel Tower is one of the most visited monuments in the world[1].\"\n",
    "- Use multiple sources for a single detail if applicable, e.g., \"Paris is a cultural hub, attracting millions of visitors each year[1][2].\"\n",
    "- Always prioritize credibility and accuracy by linking all statements to their respective sources where applicable.\n",
    "\n",
    "## Special Instructions\n",
    "- If the query involves technical, historical, or complex topics, provide detailed sections of context and explanation to ensure clarity.\n",
    "- If the user provides a vague query or lacks relevant information, explain what additional details could help refine the search.\n",
    "- If no relevant information is found, state: \"Hmm, sorry, I couldn't find any relevant information on this topic. Would you like to rephrase your query?\" Be transparent about limitations and suggest alternatives or ways to rephrase the query.\n",
    "\n",
    "## Example Output\n",
    "- Start with a sharp, strategic overview that directly ties the key insights from the sources to the client’s business objectives. Ensure the context is clear, focused, and aligned with the client’s overarching goals in the tender process, highlighting only the most impactful elements.\n",
    "- Deliver a thorough and structured analysis, breaking down each relevant facet of the query with precision. Provide actionable, high-value recommendations that drive the client’s decision-making, considering not just immediate compliance, but also long-term competitive positioning, risk mitigation, and strategic alignment.\n",
    "- Where necessary, offer brief yet clear explanations to make complex or technical information easily digestible. The goal is to ensure that the client can swiftly translate the insights into concrete actions with a clear understanding of their strategic significance.\n",
    "- Conclude with a focused, strategic summary that crystallizes the core takeaways, positioning them within the broader business context. Propose next steps that are both practical and strategically impactful, guiding the client toward concrete actions that refine their tender approach or enhance their overall strategy.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "main_prompt = ChatPromptTemplate.from_template(MAIN_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4335e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import tiktoken\n",
    "\n",
    "def get_context(docs: List[Dict]) -> str:\n",
    "    \"\"\"Construit le contexte à partir des documents en respectant la limite de tokens\"\"\"\n",
    "    encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    context = \"\"\n",
    "    max_context_size = 12288  \n",
    "    for idx, doc in enumerate(docs):\n",
    "        doc_text = f\"{idx + 1}. {doc['text']}, \\n Number {idx + 1}, \\nSource: {doc['doc_name']}, Page {doc['page']}\\n\\n\"\n",
    "        new_context = context + doc_text\n",
    "        if len(encoder.encode(new_context)) < max_context_size:\n",
    "            context = new_context\n",
    "        else:\n",
    "            tokens = encoder.encode(doc['text'])\n",
    "            remaining_tokens = max_context_size - len(encoder.encode(context))\n",
    "            truncated_text = encoder.decode(tokens[:remaining_tokens]) + \" [TRUNCATED]\"\n",
    "            context += f\"{idx + 1}. {truncated_text}\\nSource: {doc['doc_name']}, Page {doc['page']}\\n\\n\"\n",
    "            break\n",
    "    return context\n",
    "\n",
    "def get_response_with_sources(retriever, query: str, api_key: str) -> tuple[str, List[Document]]:\n",
    "    \"\"\"Retourne la réponse générée et les documents sources pertinents\"\"\"\n",
    "    sources = retriever.get_relevant_documents(query)\n",
    "    docs_for_context = []\n",
    "    for i, doc in enumerate(sources):\n",
    "        docs_for_context.append({\n",
    "            'text': doc.page_content,\n",
    "            'doc_name': doc.metadata.get('doc', 'Unknown'),\n",
    "            'page': doc.metadata.get('page', 'N/A')\n",
    "        })\n",
    "    context_str = get_context(docs_for_context)\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash-latest\",\n",
    "        google_api_key=api_key,\n",
    "        temperature=0.3,\n",
    "        max_output_tokens=2048\n",
    "    )\n",
    "    chain = (\n",
    "        {\"context\": lambda x: context_str, \"question\": lambda x: x[\"question\"]}\n",
    "        | main_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    response = chain.invoke({\"question\": query})\n",
    "    docs_sources = [{\n",
    "        \"id\": doc.metadata.get(\"id\", \"Unknown\"),\n",
    "        \"page\": doc.metadata.get(\"page\", \"N/A\"),\n",
    "        \"doc_name\": doc.metadata.get(\"doc\", \"Unknown\"),\n",
    "        \"text\": doc.page_content\n",
    "    } for doc in sources\n",
    "    ]\n",
    "    final_sources = get_final_sources_used(response, docs_sources)\n",
    "    return response, final_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad0cc92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ocr_languages kwarg will be deprecated in a future version of unstructured. Please use languages instead.\n",
      "Only one of languages and ocr_languages should be specified. languages is preferred. ocr_languages is marked for deprecation.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "elements = ocr_pipeline(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac5b05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'chunk_1_CV_2025-04-14_FOFANA_Ibrahim seloh',\n",
       "  'page': 1,\n",
       "  'text': \"ibrahim.fofana@central e-casablanca.ma\\n«\\nhttps://www.linkedin.co m/in/ibrahim-seloh- fofana-6073b4291 g\\nCasablanca, Morocco >\\n+212 694 419496 @\\nTélétravail ou présentiel 0\\n°o Casablanca, Morocco\\nATOUTS TECHNIQUES\\nDéveloppement et programmation : Python, SQL, C, C++, HTML, CSS, JavaScript, VBA, LangChain, GIT, Quandl\\nData science, analyse et outils stratégiques : MATLAB, SCILAB, Power BI, Deep learning, Web scraping, Docker, SWOT, PESTEL, analyse de marché, stratégie marketing, pricing\\n00000\\nBureautique et gestion : Pack Office (Word, Excel, PowerPoint), WordPress, gestion de projet\\nCentres d'intérêt\\nBasket Ball - Lecture - Film - Musique\\nLangues\\nFrançais Niveau Cl\\nAnglais Niveau B2\\nFOFANA IBRAHIM SELOH\\nEtudiant-ingénieur à l'École Centrale Casablanca, je combine rigueur analytique, expertise en data science et passion pour l'IA afin de résoudre des problématiques complexes et stratégiques. Je suis à la recherche d'un stage césure en Data Science, Gen Al, NLP ou Data Analytics, disponible dès avril 2025.\\nDiplômes et Formations\\nIngénieur généraliste\\nDepuis septembre 2022 , Ecole Centrale Casablanca, Casablanca, Morocco\\nClasses préparatoires aux grandes écoles\\nDe septembre 2020 à juillet 2022\\nInstitut National Polytechnique Houphouét Boigny Yamoussoukro, Côte d'Ivoire\\nBaccalauréat scientifique\\nDe septembre 2019 à juillet 2020 Lycée Classique de Bouaké Bouaké, Côte d'ivoire\\nExpériences professionnelles\\nData scientist\\nDe septembre 2024 à février 2025 DATA 354 Abidjan, AB, Ivory Coast\\ne Création et gestion de bases de données juridiques et financiéres avec optimisation des données.\\n+ Développement du chatbot ALIA pour l'accès aux articles de loi et l'assistance au corps juridique avec des réponses contextuelles.\\ne Mise en place du système RAG (BRVM Report Analyser) pour l'analyse des états financiers des entreprises cotées, offrant une aide stratégique aux traders.\\nConsultant en stratégie et transformation digitale\\nDepuis juin 2024 R2IFE Casablanca, Morocco\\nStratégie consulting de l'entreprise GOMA Service :\\ne Analyse stratégique et accompagnement de l'entreprise GOMA Service via des études SWOT/PESTEL, la création d'outils de suivi des performances, et la mise en place d'une plateforme de vente par services.\\nData scientist\\nDe mai 2024 à septembre 2024 Fondation MASCIR- UM6P Casablanca\\ne Développement d'un Chatbot pour personnes âgées en maison de retraite avec Rasa Framework\\ne Optimisation via système RAG avec LangChain, Ollama, et Hugging-Face\\nAnalyste financier\\nDe juillet 2023 à septembre 2023 IDUS CAPITAL Casablanca\\ne Analyse du marché et des risques financiers dans l'industrie des matériaux plastiques. e Prévisions de rentabilité et Recommandations d'investissement fondées sur ROI et évaluation des risques\\nProjets academiques\\nProjet Scientifique ML ( Octobre 2023 - Decembre 2023)\\n+ Mission : Développement d'un modèle de prédiction de rendement agricole pour orienter les décisions stratégiques des acteurs du secteur agricole.\\nProjet Learning By Doing (Septembre 2022 - Juillet 2023)\\n+ Mission : Développement d'un système de tri automatique de déchets par reconnaissance d'image à l'aide du Machine Learning\\nCompétences\\nAnalyse et conseil stratégique\\n\\nRéalisation d'analyses stratégiques approfondies (SWOT, PESTEL, analyse concurrentielle)\\n¢ Élaboration de recommandations stratégiques pour l'optimisation des performances organisationnelles\\nAnalyse financière, gestion des risques et data science\\ne Évaluation de la rentabilité et des risques financiers liés aux investissements, avec modélisation et prédiction des performances via des modèles ML.\\ne Analyse des états financiers et traitement de données complexes pour la prise de décision stratégique\",\n",
       "  'doc': 'CV_2025-04-14_FOFANA_Ibrahim seloh'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3508e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index_pdf_elements(elements, GOOGLE_API_KEY, \"Doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4836f547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6936/1994336233.py:26: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  sources = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Principaux Projets de Data Science\n",
      "\n",
      "Le document présente dix projets de data science, chacun axé sur des problématiques et des techniques spécifiques.  Voici un résumé des principaux projets:\n",
      "\n",
      "### 1. Analyse Exploratoire des Données (EDA) [1, 2]\n",
      "\n",
      "* **Objectif:** Comprendre la structure et la distribution des données, identifier les variables importantes et leurs relations, et produire des visualisations claires.\n",
      "* **Problématiques:**  Analyse de transactions bancaires pour identifier la fraude et segmentation de clients rentables en marketing.\n",
      "* **Technologies:** Python (Pandas, Matplotlib/Seaborn, NumPy), Shiny/Streamlit/Dash.\n",
      "\n",
      "### 2. A/B Testing [13, 14]\n",
      "\n",
      "* **Objectif:** Comparer deux versions d'un produit ou service pour déterminer la plus performante.\n",
      "* **Problématique:** Déterminer quelle version d'une landing page convertit le mieux les utilisateurs.\n",
      "* **Technologies:** Python (Pandas, Scipy), Matplotlib/Seaborn.\n",
      "\n",
      "### 3. Clustering et Réduction de Dimensions [17, 18]\n",
      "\n",
      "* **Objectif:** Segmenter des données en groupes homogènes et visualiser les clusters.\n",
      "* **Problématique:** Segmenter les clients en fonction de leurs comportements d'achat.\n",
      "* **Technologies:** Python (Scikit-learn, t-SNE).\n",
      "\n",
      "### 4. Modélisation Prédictive avec un Classificateur [15, 16]\n",
      "\n",
      "* **Objectif:** Créer un modèle prédictif et comparer les performances de différents modèles de classification.\n",
      "* **Problématique:** Identifier les clients à risque de défaut de paiement dans le secteur bancaire.\n",
      "* **Technologies:** Python (Scikit-learn, Flask/FastAPI, Docker),  divers modèles de régression et classification (Régression Logistique, SVM, KNN, Naive Bayes, Lasso Regression, ElasticNet, Random Forest, AdaBoost, XGBoost, LightGBM, CatBoost).\n",
      "\n",
      "### 5. Analyse des Séries Temporelles [9, 10]\n",
      "\n",
      "* **Objectif:** Prédire les tendances futures à partir de données dépendantes du temps.\n",
      "* **Problématique:** Prédire le volume des transactions bancaires.\n",
      "* **Technologies:** Python (Statsmodels, Prophet, XGBoost, LSTM, Flask/FastAPI).\n",
      "\n",
      "### 6. Deep Learning - Classification Cat vs Dog [3, 4]\n",
      "\n",
      "* **Objectif:** Implémenter un modèle de deep learning pour classifier des images de chats et de chiens.\n",
      "* **Problématique:** Classification d'images de chats et de chiens dans l'industrie.\n",
      "* **Technologies:** Python (TensorFlow, Keras, OpenCV), modèles CNN (VGG16, ResNet, InceptionV3), Flask/FastAPI, Docker, Kubernetes.\n",
      "\n",
      "### 7. IA Générative [7, 8]\n",
      "\n",
      "* **Objectif:** Implémenter un GAN pour générer des images réalistes (vêtements).\n",
      "* **Problématique:** Génération d'images réalistes, notamment de vêtements.\n",
      "* **Technologies:**  Techniques de Transfert Learning et Fine-Tuning,  déploiement d'API et mise en production avec Docker et Kubernetes.\n",
      "\n",
      "### 8. Systèmes de Recommandation [5, 6]\n",
      "\n",
      "* **Objectif:**  Développer un système de recommandation pour personnaliser l'expérience utilisateur.\n",
      "* **Problématique:**  Fournir des recommandations personnalisées de produits ou de contenus.\n",
      "* **Technologies:** API de modèles de langage génératifs (GPT, LLaMA), Flask/FastAPI, Docker, Kubernetes.\n",
      "\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Ces projets illustrent une large gamme de compétences en data science, couvrant l'analyse exploratoire, la modélisation prédictive, le deep learning, et le déploiement d'applications à grande échelle.  Ils démontrent une maîtrise des outils et techniques nécessaires pour résoudre des problèmes concrets dans différents secteurs d'activité.  L'accent mis sur le déploiement via Docker et Kubernetes souligne une orientation vers des solutions industrielles et scalables.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, sources = get_response_with_sources(retriever, \"Quels sont les principaux projets de data science présentés dans le document ?\", GOOGLE_API_KEY)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d60ce81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '1',\n",
       "  'page': 3,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"Projet 1 : Analyse Exploratoire des Données (EDA)\\nContexte\\nL'analyse exploratoire des données (EDA) est une étape clé dans tout projet de data science. Ce projet permet de montrer votre capacité à comprendre des données brutes, à les nettoyer, et à formuler des hypothèses avant d'appliquer des modèles plus complexes.\\nObjectifs\\ne Comprendre la structure des données et leur distribution.\\ne Identifier les variables importantes et les relations entre elles.\\ne Produire des visualisations claires pour présenter les résultats.\\nProblematiques\\nBanque : Comment analyser les transactions bancaires pour identifier des schémas de fraude ?\\n- Dataset : https://www.kaggle.com/datasets/mlg- ulb/creditcardfraud\\nMarketing : Quels sont les segments de clients les plus rentables en fonction de leurs interactions avec des campagnes ?\\n- Dataset : https://www.kaggle.com/datasets/rodsaldanha/arketing- campaign/data\\nTechnologies Utilisées\\ne Python\\nPandas : Pour la manipulation et la transformation des données.\\nMatplotlib/Seaborn : Pour visualiser les tendances a travers des graphiques comme les histogrammes, scatter plots, et heatmaps.\\nNumPy : Pour les calculs mathématiques et statistiques.\\nShiny/Streamlit/Dash : Pour déployer un tableau de bord interactif afin de présenter les résultats de l'EDA aux parties prenantes.\\ne Statistique Univariée : Analyse d'une seule variable pour en comprendre la distribution et les caractéristiques.\\ne Statistique Multivariée : Analyse des relations entre plusieurs variables afin d'explorer des corrélations ou des interactions.\\nee)\"},\n",
       " {'id': '2',\n",
       "  'page': 3,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"Projet 1 : Analyse Exploratoire des Données (EDA)\\nContexte\\nL'analyse exploratoire des données (EDA) est une étape clé dans tout projet de data science. Ce projet permet de montrer votre capacité à comprendre des données brutes, à les nettoyer, et à formuler des hypothèses avant d'appliquer des modèles plus complexes.\\nObjectifs\\ne Comprendre la structure des données et leur distribution.\\ne Identifier les variables importantes et les relations entre elles.\\ne Produire des visualisations claires pour présenter les résultats.\\nProblematiques\\nBanque : Comment analyser les transactions bancaires pour identifier des schémas de fraude ?\\n- Dataset : https://www.kaggle.com/datasets/mlg- ulb/creditcardfraud\\nMarketing : Quels sont les segments de clients les plus rentables en fonction de leurs interactions avec des campagnes ?\\n- Dataset : https://www.kaggle.com/datasets/rodsaldanha/arketing- campaign/data\\nTechnologies Utilisées\\ne Python\\nPandas : Pour la manipulation et la transformation des données.\\nMatplotlib/Seaborn : Pour visualiser les tendances a travers des graphiques comme les histogrammes, scatter plots, et heatmaps.\\nNumPy : Pour les calculs mathématiques et statistiques.\\nShiny/Streamlit/Dash : Pour déployer un tableau de bord interactif afin de présenter les résultats de l'EDA aux parties prenantes.\\ne Statistique Univariée : Analyse d'une seule variable pour en comprendre la distribution et les caractéristiques.\\ne Statistique Multivariée : Analyse des relations entre plusieurs variables afin d'explorer des corrélations ou des interactions.\\nee)\"},\n",
       " {'id': '3',\n",
       "  'page': 10,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"e Docker et Kubernetes : Containerisation et orchestration des modeles pour les déployer sur des plateformes cloud.\\nCompétences Acquises\\ne Modélisation des séries temporelles : Utilisation de modèles ARIMA, SARIMA et Prophet pour capturer les tendances temporelles et saisonnières.\\ne Déploiement d’une API de prévision : Construction d’une API pour fournir des prévisions continues\\ne Orchestration avec Kubernetes : Gestion des déploiements à grande échelle avec Docker et Kubernetes\\nProjet 7 : Deep Learning - Classification Cat vs Dog\\nContexte\\nCe projet montre comment implémenter un modèle de deep learning pour classer des images de chats et de chiens\\nObjectifs\\ne Implémenter un modèle de deep learning.\\ne Optimiser les hyperparametres.\\nProblematiques\\nIndustrie : Comment classifier des images de chats et de chiens ? - Dataset : https://www.kaggle.com/c/dogs-vs-cats/data\\nTechnologies Utilisées\\ne Python : TensorFlow, Keras, OpenCV\\ne Modèles :\\n- CNN (Convolutional Neural Networks) : Pour la classification d'images - VGG16, ResNet, InceptionV3 : Modèles préentrainés utilisés pour le transfert learning\\ne Flask/FastAPI : Déploiement du modèle sous forme d'API pour prédictions en temps réel.\\nDocker : Containerisation pour déploiement en production\\ne\\ne Kubernetes : Orchestration des conteneurs pour une gestion a grande échelle\\nee)\"},\n",
       " {'id': '4',\n",
       "  'page': 10,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"e Docker et Kubernetes : Containerisation et orchestration des modeles pour les déployer sur des plateformes cloud.\\nCompétences Acquises\\ne Modélisation des séries temporelles : Utilisation de modèles ARIMA, SARIMA et Prophet pour capturer les tendances temporelles et saisonnières.\\ne Déploiement d’une API de prévision : Construction d’une API pour fournir des prévisions continues\\ne Orchestration avec Kubernetes : Gestion des déploiements à grande échelle avec Docker et Kubernetes\\nProjet 7 : Deep Learning - Classification Cat vs Dog\\nContexte\\nCe projet montre comment implémenter un modèle de deep learning pour classer des images de chats et de chiens\\nObjectifs\\ne Implémenter un modèle de deep learning.\\ne Optimiser les hyperparametres.\\nProblematiques\\nIndustrie : Comment classifier des images de chats et de chiens ? - Dataset : https://www.kaggle.com/c/dogs-vs-cats/data\\nTechnologies Utilisées\\ne Python : TensorFlow, Keras, OpenCV\\ne Modèles :\\n- CNN (Convolutional Neural Networks) : Pour la classification d'images - VGG16, ResNet, InceptionV3 : Modèles préentrainés utilisés pour le transfert learning\\ne Flask/FastAPI : Déploiement du modèle sous forme d'API pour prédictions en temps réel.\\nDocker : Containerisation pour déploiement en production\\ne\\ne Kubernetes : Orchestration des conteneurs pour une gestion a grande échelle\\nee)\"},\n",
       " {'id': '5',\n",
       "  'page': 15,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"Compétences Acquises\\ne Utilisation des API de modèles de langage génératifs : Maîtrise de l'intégration des API de GPT, LLaMA, ou autres modèles sur Hugging Face pour générer du texte.\\ne Optimisation des prompts et fine-tuning des modèles : Ajustement des prompts et personnalisation des modèles pour répondre à des besoins métiers spécifiques.\\ne Déploiement d’une application basée sur un modèle génératif : Utilisation de Flask/FastAPI pour déployer une API capable de générer du texte en temps réel.\\ne Mise en production et scalabilité : Utilisation de Docker et Kubernetes pour la mise en production et la gestion d'une infrastructure scalable dans le cloud.\\nEtapes du projet\\n1. Exploration des API disponibles : S'approprier l'API de GPT-4 via OpenAl ou l'API Hugging Face pour LLaMA et autres modèles. Tester des exemples de génération de texte.\\n2. Conception d'un pipeline de prompts : Définir différents prompts et scénarios pour la génération automatique de contenu.\\n3. Fine-tuning et personnalisation : Si nécessaire, affiner le modele sur un jeu de données spécifique pour améliorer la qualité des réponses.\\n4. Création d'une interface interactive : Utiliser Streamlit ou Gradio pour permettre aux utilisateurs de tester l'API en temps réel via une interface simple.\\n5. Déploiement en production : Déployer l'API et l'interface utilisateur à l’aide de Docker et Kubernetes pour permettre une utilisation à grande échelle\\nProjet 10 : Systemes de Recommandation\\nContexte\\nLes systèmes de recommandation sont au cœur de nombreuses plateformes modernes telles que Netflix, Amazon, ou Spotify. Ces systèmes permettent de personnaliser l'expérience utilisateur en proposant des produits ou des contenus en fonction de leurs préférences et comportements passés.\\nee)\"},\n",
       " {'id': '6',\n",
       "  'page': 15,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"Compétences Acquises\\ne Utilisation des API de modèles de langage génératifs : Maîtrise de l'intégration des API de GPT, LLaMA, ou autres modèles sur Hugging Face pour générer du texte.\\ne Optimisation des prompts et fine-tuning des modèles : Ajustement des prompts et personnalisation des modèles pour répondre à des besoins métiers spécifiques.\\ne Déploiement d’une application basée sur un modèle génératif : Utilisation de Flask/FastAPI pour déployer une API capable de générer du texte en temps réel.\\ne Mise en production et scalabilité : Utilisation de Docker et Kubernetes pour la mise en production et la gestion d'une infrastructure scalable dans le cloud.\\nEtapes du projet\\n1. Exploration des API disponibles : S'approprier l'API de GPT-4 via OpenAl ou l'API Hugging Face pour LLaMA et autres modèles. Tester des exemples de génération de texte.\\n2. Conception d'un pipeline de prompts : Définir différents prompts et scénarios pour la génération automatique de contenu.\\n3. Fine-tuning et personnalisation : Si nécessaire, affiner le modele sur un jeu de données spécifique pour améliorer la qualité des réponses.\\n4. Création d'une interface interactive : Utiliser Streamlit ou Gradio pour permettre aux utilisateurs de tester l'API en temps réel via une interface simple.\\n5. Déploiement en production : Déployer l'API et l'interface utilisateur à l’aide de Docker et Kubernetes pour permettre une utilisation à grande échelle\\nProjet 10 : Systemes de Recommandation\\nContexte\\nLes systèmes de recommandation sont au cœur de nombreuses plateformes modernes telles que Netflix, Amazon, ou Spotify. Ces systèmes permettent de personnaliser l'expérience utilisateur en proposant des produits ou des contenus en fonction de leurs préférences et comportements passés.\\nee)\"},\n",
       " {'id': '7',\n",
       "  'page': 11,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"Transfert Learning et Fine-Tuning\\nTransfert Learning : Utilisation de modèles préentraînés sur de grands ensembles de données (comme ImageNet) pour initialiser les poids du modèle, ce qui permet d'accélérer l'entraînement et d'améliorer la précision avec peu de données.\\nFine-Tuning : Réentrainement des couches supérieures du modèle preentrainé pour ajuster les paramètres aux nouvelles données spécifiques au projet (exemple : classifier des images de chats et de chiens).\\nCompétences Acquises\\nImplémentation de réseaux de neurones convolutionnels (CNN) : Capacité à construire et entraîner des CNNs pour la classification d'images.\\nTransfert Learning et Fine-Tuning : Application de techniques avancées pour améliorer les performances du modèle en utilisant des modèles préentraînés.\\nOptimisation des hyperparamètres : Utilisation de Grid Search et Random Search pour ajuster les paramètres du modèle.\\nDéploiement d'un modèle de deep learning sous forme d'API : Capacité à créer une API REST qui fournit des prédictions en temps réel.\\nMise en production avec Docker et Kubernetes : Containerisation des modèles pour une gestion et une mise en production évolutive\\nProjet 8 : IA Générative\\nContexte\\nL'IA générative est une technologie révolutionnaire qui permet de créer des images, du texte, ou même de la musique à partir de données d'entraînement. Ce projet vise à implémenter un GAN (Generative Adversarial Network) pour générer des images réalistes, en particulier des vêtements à partir du dataset Fashion MNIST. Le projet couvre les aspects fondamentaux de l'entraînement de modèles génératifs et leur optimisation, ainsi que leur mise en production pour des applications en temps réel\"},\n",
       " {'id': '8',\n",
       "  'page': 11,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"Transfert Learning et Fine-Tuning\\nTransfert Learning : Utilisation de modèles préentraînés sur de grands ensembles de données (comme ImageNet) pour initialiser les poids du modèle, ce qui permet d'accélérer l'entraînement et d'améliorer la précision avec peu de données.\\nFine-Tuning : Réentrainement des couches supérieures du modèle preentrainé pour ajuster les paramètres aux nouvelles données spécifiques au projet (exemple : classifier des images de chats et de chiens).\\nCompétences Acquises\\nImplémentation de réseaux de neurones convolutionnels (CNN) : Capacité à construire et entraîner des CNNs pour la classification d'images.\\nTransfert Learning et Fine-Tuning : Application de techniques avancées pour améliorer les performances du modèle en utilisant des modèles préentraînés.\\nOptimisation des hyperparamètres : Utilisation de Grid Search et Random Search pour ajuster les paramètres du modèle.\\nDéploiement d'un modèle de deep learning sous forme d'API : Capacité à créer une API REST qui fournit des prédictions en temps réel.\\nMise en production avec Docker et Kubernetes : Containerisation des modèles pour une gestion et une mise en production évolutive\\nProjet 8 : IA Générative\\nContexte\\nL'IA générative est une technologie révolutionnaire qui permet de créer des images, du texte, ou même de la musique à partir de données d'entraînement. Ce projet vise à implémenter un GAN (Generative Adversarial Network) pour générer des images réalistes, en particulier des vêtements à partir du dataset Fashion MNIST. Le projet couvre les aspects fondamentaux de l'entraînement de modèles génératifs et leur optimisation, ainsi que leur mise en production pour des applications en temps réel\"},\n",
       " {'id': '9',\n",
       "  'page': 9,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"e Modèles d'ensemble (Ensemble Learning)\\n- Random Forest : Un ensemble d’arbres de décision pour améliorer la precision\\n- Gradient Boosting Machines (GBM) : Modèles successifs corrigeant les erreurs des précédents\\n- XGBoost : Variante rapide et efficace du GBM, couramment utilisée en production\\n- CatBoost et LightGBM : Modeles basés sur le boosting, optimisés pour la vitesse et la performance\\ne Déploiement d'une API prédictive\\ne Docker : Containerisation des modèles pour faciliter leur mise en production sur le cloud (AWS, GCP, Azure)\\nProjet 6 : Analyse des Series Temporelles\\nContexte\\nL’analyse des séries temporelles permet de modéliser des données dépendantes du temps.\\nObjectifs\\ne Prédire les tendances futures\\nProblematiques\\nBanque : Comment prédire le volume des transactions bancaires ? - Dataset : https://www.kaggle.com/datasets/mczielinski/bitcoin- historical-data\\nTechnologies Utilisées\\ne Python:\\n- Statsmodels : Implémentation des modeles ARIMA et SARIMA.\\n- Prophet (Facebook) : Pour la prévision des tendances saisonnières et des événements irréguliers.\\n- XGBoost : Modèle de boosting pour les séries temporelles, efficace pour capturer des relations complexes dans Les données.\\n- LSTM (Long Short-Term Memory) : Réseau de neurones récurrent utilisé pour modéliser des séries temporelles à long terme et des dépendances complexes.\\n- Flask/FastAPI : Déploiement d’une API REST qui fournit des prévisions basées sur les données historiques.\\nee)\"},\n",
       " {'id': '10',\n",
       "  'page': 9,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"e Modèles d'ensemble (Ensemble Learning)\\n- Random Forest : Un ensemble d’arbres de décision pour améliorer la precision\\n- Gradient Boosting Machines (GBM) : Modèles successifs corrigeant les erreurs des précédents\\n- XGBoost : Variante rapide et efficace du GBM, couramment utilisée en production\\n- CatBoost et LightGBM : Modeles basés sur le boosting, optimisés pour la vitesse et la performance\\ne Déploiement d'une API prédictive\\ne Docker : Containerisation des modèles pour faciliter leur mise en production sur le cloud (AWS, GCP, Azure)\\nProjet 6 : Analyse des Series Temporelles\\nContexte\\nL’analyse des séries temporelles permet de modéliser des données dépendantes du temps.\\nObjectifs\\ne Prédire les tendances futures\\nProblematiques\\nBanque : Comment prédire le volume des transactions bancaires ? - Dataset : https://www.kaggle.com/datasets/mczielinski/bitcoin- historical-data\\nTechnologies Utilisées\\ne Python:\\n- Statsmodels : Implémentation des modeles ARIMA et SARIMA.\\n- Prophet (Facebook) : Pour la prévision des tendances saisonnières et des événements irréguliers.\\n- XGBoost : Modèle de boosting pour les séries temporelles, efficace pour capturer des relations complexes dans Les données.\\n- LSTM (Long Short-Term Memory) : Réseau de neurones récurrent utilisé pour modéliser des séries temporelles à long terme et des dépendances complexes.\\n- Flask/FastAPI : Déploiement d’une API REST qui fournit des prévisions basées sur les données historiques.\\nee)\"},\n",
       " {'id': '13',\n",
       "  'page': 4,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"Compétences Acquises\\ne Manipulation des données : Capacité à manipuler de larges volumes de données avec Pandas et NumPy.\\ne Nettoyage des données : Utilisation de méthodes d'identification des valeurs manquantes, gestion des doublons et transformation des variables catégorielles.\\ne Visualisation des données : Création de visualisations perspicaces avec Matplotlib et Seaborn pour communiquer efficacement les résultats.\\ne Statistique Univariée : Capacité a analyser les caractéristiques d'une variable individuelle.\\ne Statistique Multivariée : Capacité a analyser et interpréter les relations entre plusieurs variables.\\ne Déploiement d’un tableau de bord interactif : Création d'applications de data visualisation avec Streamlit, Dash, ou Shiny pour fournir des insights interactifs et accessibles aux non-techniciens.\\nCes projets vous inspirent ? Suivez-moi sur mes différents réseaux pour ne pas rater Les prochains.\\nProjet 2 : A/B Testing\\nContexte\\nL’A/B testing est une méthode utilisée pour comparer deux versions d'un produit ou d'un service afin de déterminer laquelle est la plus performante. Ce projet est très populaire dans le marketing, les produits digitaux et l'e-commerce pour optimiser des campagnes ou des fonctionnalités.\\nObjectifs\\ne Mettre en place une expérimentation contrôlée pour comparer deux groupes\\ne Analyser les résultats avec des statistiques pour choisir la version gagnante\"},\n",
       " {'id': '14',\n",
       "  'page': 4,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"Compétences Acquises\\ne Manipulation des données : Capacité à manipuler de larges volumes de données avec Pandas et NumPy.\\ne Nettoyage des données : Utilisation de méthodes d'identification des valeurs manquantes, gestion des doublons et transformation des variables catégorielles.\\ne Visualisation des données : Création de visualisations perspicaces avec Matplotlib et Seaborn pour communiquer efficacement les résultats.\\ne Statistique Univariée : Capacité a analyser les caractéristiques d'une variable individuelle.\\ne Statistique Multivariée : Capacité a analyser et interpréter les relations entre plusieurs variables.\\ne Déploiement d’un tableau de bord interactif : Création d'applications de data visualisation avec Streamlit, Dash, ou Shiny pour fournir des insights interactifs et accessibles aux non-techniciens.\\nCes projets vous inspirent ? Suivez-moi sur mes différents réseaux pour ne pas rater Les prochains.\\nProjet 2 : A/B Testing\\nContexte\\nL’A/B testing est une méthode utilisée pour comparer deux versions d'un produit ou d'un service afin de déterminer laquelle est la plus performante. Ce projet est très populaire dans le marketing, les produits digitaux et l'e-commerce pour optimiser des campagnes ou des fonctionnalités.\\nObjectifs\\ne Mettre en place une expérimentation contrôlée pour comparer deux groupes\\ne Analyser les résultats avec des statistiques pour choisir la version gagnante\"},\n",
       " {'id': '15',\n",
       "  'page': 7,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"- Lasso Regression : Régression linéaire avec réduction des coefficients non pertinents\\n- ElasticNet : Combinaison de Ridge et Lasso pour optimiser les modeles avec de nombreuses variables\\n- Modèles basés sur Les arbres de décision :\\n- Arbre de décision pour la régression : Simple, explique facilement les relations complexes.\\n- Random Forest : Combinaison d'arbres de décision pour réduire le surapprentissage et améliorer la précision\\n- AdaBoost : Méthode de boosting qui améliore les performances des modeles faibles\\n- XGBoost : Algorithme de boosting populaire et efficace pour la régression\\n- LightGBM : Version optimisée pour de très grands ensembles de données\\n- CatBoost : Spécialisé dans les données catégorielles et performant dans les situations complexes\\ne Déploiement d’une API de régression : Mise en place d’une API pour fournir des prédictions en temps réel\\ne Docker : Containerisation des modèles pour faciliter leur mise en production sur Le cloud (AWS, GCP, Azure)\\nProjet 5 : Modélisation Predictive avec un Classificateur\\nContexte\\nLes entreprises utilisent les données pour prédire des événements futurs tels que Le défaut de paiement.\\nObjectifs\\ne Créer un modèle prédictif.\\ne Comparer les performances de différents modèles de classification.\\nProblématiques\\nBanque : Quels clients risquent de faire défaut sur leurs prêts ?\\nContexte : Dans le secteur bancaire, il est crucial d'identifier les clients à risque pour minimiser les pertes liées aux prêts non remboursés. Ce projet vise à créer un modèle de classification qui prédit La probabilité qu'un client fasse défaut\\n- Dataset : https://www.kaggle.com/datasets/itsmesunil/bank-loan- modelling\\nmm\"},\n",
       " {'id': '16',\n",
       "  'page': 7,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"- Lasso Regression : Régression linéaire avec réduction des coefficients non pertinents\\n- ElasticNet : Combinaison de Ridge et Lasso pour optimiser les modeles avec de nombreuses variables\\n- Modèles basés sur Les arbres de décision :\\n- Arbre de décision pour la régression : Simple, explique facilement les relations complexes.\\n- Random Forest : Combinaison d'arbres de décision pour réduire le surapprentissage et améliorer la précision\\n- AdaBoost : Méthode de boosting qui améliore les performances des modeles faibles\\n- XGBoost : Algorithme de boosting populaire et efficace pour la régression\\n- LightGBM : Version optimisée pour de très grands ensembles de données\\n- CatBoost : Spécialisé dans les données catégorielles et performant dans les situations complexes\\ne Déploiement d’une API de régression : Mise en place d’une API pour fournir des prédictions en temps réel\\ne Docker : Containerisation des modèles pour faciliter leur mise en production sur Le cloud (AWS, GCP, Azure)\\nProjet 5 : Modélisation Predictive avec un Classificateur\\nContexte\\nLes entreprises utilisent les données pour prédire des événements futurs tels que Le défaut de paiement.\\nObjectifs\\ne Créer un modèle prédictif.\\ne Comparer les performances de différents modèles de classification.\\nProblématiques\\nBanque : Quels clients risquent de faire défaut sur leurs prêts ?\\nContexte : Dans le secteur bancaire, il est crucial d'identifier les clients à risque pour minimiser les pertes liées aux prêts non remboursés. Ce projet vise à créer un modèle de classification qui prédit La probabilité qu'un client fasse défaut\\n- Dataset : https://www.kaggle.com/datasets/itsmesunil/bank-loan- modelling\\nmm\"},\n",
       " {'id': '17',\n",
       "  'page': 5,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"Problématiques\\nMarketing : Quelle version d'une landing page convertit le mieux les utilisateurs ?\\n- Dataset : https://www.kaggle.com/datasets/zhangluyuan/ab-testing\\nTechnologies Utilisées\\ne Python : Pandas, Scipy (tests statistiques)\\ne Matplotlib/Seaborn : Pour visualiser les résultats\\nCompetences Acquises\\ne Tests statistiques\\ne Analyse statistique des résultats\\ne Visualisation des résultats pour la prise de décision\\nProjet 3 : Clustering et Reduction de Dimensions\\nContexte\\nLe clustering permet de segmenter des données en groupes homogènes, utile pour la segmentation des clients.\\nObjectifs\\ne Appliquer des techniques de clustering\\ne Utiliser des techniques de réduction de dimension pour visualiser les clusters\\nProblématiques\\nMarketing : Comment segmenter les clients en fonction de leurs comportements d'achat ?\\n- Dataset :\\nhttps://www.kaggle.com/datasets/kaushiksuresh147/customer- segmentation\\nTechnologies Utilisées\\ne Python : Scikit-learn (KMeans, PCA), t-SNE\"},\n",
       " {'id': '18',\n",
       "  'page': 5,\n",
       "  'doc_name': '10 Projets pour un Portfolio Data Science RÃ©ussi ',\n",
       "  'text': \"Problématiques\\nMarketing : Quelle version d'une landing page convertit le mieux les utilisateurs ?\\n- Dataset : https://www.kaggle.com/datasets/zhangluyuan/ab-testing\\nTechnologies Utilisées\\ne Python : Pandas, Scipy (tests statistiques)\\ne Matplotlib/Seaborn : Pour visualiser les résultats\\nCompetences Acquises\\ne Tests statistiques\\ne Analyse statistique des résultats\\ne Visualisation des résultats pour la prise de décision\\nProjet 3 : Clustering et Reduction de Dimensions\\nContexte\\nLe clustering permet de segmenter des données en groupes homogènes, utile pour la segmentation des clients.\\nObjectifs\\ne Appliquer des techniques de clustering\\ne Utiliser des techniques de réduction de dimension pour visualiser les clusters\\nProblématiques\\nMarketing : Comment segmenter les clients en fonction de leurs comportements d'achat ?\\n- Dataset :\\nhttps://www.kaggle.com/datasets/kaushiksuresh147/customer- segmentation\\nTechnologies Utilisées\\ne Python : Scikit-learn (KMeans, PCA), t-SNE\"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57c03378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Les documents présentent plusieurs projets de data science, chacun axé sur des objectifs et des technologies spécifiques.  Voici un résumé des principaux projets:\n",
       "\n",
       "## Projet 1: Analyse Exploratoire des Données (EDA) [1, 2]\n",
       "\n",
       "Ce projet met l'accent sur l'analyse exploratoire des données, une étape cruciale dans tout projet de data science.  Il vise à comprendre la structure et la distribution des données, identifier les variables importantes et leurs relations, et présenter les résultats via des visualisations claires [1, 2].  Deux cas d'étude sont proposés : l'analyse de transactions bancaires pour détecter la fraude, utilisant le dataset disponible sur Kaggle [1, 2], et la segmentation de clients rentables basée sur l'interaction avec des campagnes marketing, utilisant un autre dataset Kaggle [1, 2]. Les technologies utilisées incluent Python, Pandas, Matplotlib/Seaborn, NumPy, et des outils de déploiement de tableaux de bord interactifs comme Shiny, Streamlit ou Dash [1, 2].  L'analyse statistique univariée et multivariée sont également employées [1, 2].\n",
       "\n",
       "## Projet 7: Deep Learning - Classification Cat vs Dog [3, 4]\n",
       "\n",
       "Ce projet se concentre sur l'implémentation d'un modèle de deep learning pour classifier des images de chats et de chiens [3, 4].  L'objectif est d'implémenter et d'optimiser un modèle de deep learning, utilisant des CNN (Convolutional Neural Networks) et des modèles pré-entraînés comme VGG16, ResNet, et InceptionV3 pour le transfert learning [3, 4].  Le dataset provient de Kaggle [3, 4].  Le projet inclut également le déploiement du modèle sous forme d'API (Flask/FastAPI), sa containerisation avec Docker, et son orchestration avec Kubernetes [3, 4].\n",
       "\n",
       "## Projet 10: Systèmes de Recommandation [5]\n",
       "\n",
       "Ce projet explore les systèmes de recommandation, un élément central de nombreuses plateformes modernes.  Il vise à personnaliser l'expérience utilisateur en proposant des produits ou contenus basés sur les préférences et comportements passés [5].  Bien que les détails techniques soient moins explicites que pour les autres projets, le document mentionne les étapes du projet, incluant l'exploration d'APIs, la conception de pipelines de prompts, le fine-tuning, la création d'interfaces interactives, et le déploiement en production avec Docker et Kubernetes [5].\n",
       "\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Les projets présentés illustrent une variété de compétences en data science, allant de l'analyse exploratoire à l'implémentation de modèles de deep learning et au déploiement d'applications à grande échelle.  Ils mettent en avant l'utilisation de différentes technologies et bibliothèques Python, ainsi que des techniques de containerisation et d'orchestration pour la mise en production.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response))\n",
    "display(Markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
